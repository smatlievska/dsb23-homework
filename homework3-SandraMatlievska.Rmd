---
title: "Homework 3: Databases, web scraping, and a basic Shiny app"
author: "Sandra Matlievska"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
library(RSQLite)
```

# Money in UK politics

[The Westminster Accounts](https://news.sky.com/story/the-westminster-accounts-12786091), a recent collaboration between Sky News and Tortoise Media, examines the flow of money through UK politics. It does so by combining data from three key sources: 

1. [Register of Members’ Financial Interests](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-members-financial-interests/), 
1. [Electoral Commission records of donations to parties](http://search.electoralcommission.org.uk/English/Search/Donations), and
1. [Register of All-Party Parliamentary Groups](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-all-party-party-parliamentary-groups/). 

You can [search and explore the results](https://news.sky.com/story/westminster-accounts-search-for-your-mp-or-enter-your-full-postcode-12771627) through the collaboration’s interactive database. Simon Willison [has extracted a database](https://til.simonwillison.net/shot-scraper/scraping-flourish) and this is what we will be working with. If you want to read more about [the project’s methodology](https://www.tortoisemedia.com/2023/01/08/the-westminster-accounts-methodology/).


## Open a connection to the database

The database made available by Simon Willison is an `SQLite` database

```{r}

sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)

```

How many tables does the database have?

```{r}

DBI::dbListTables(sky_westminster)

#It has 7 tables. 

```

## Which MP has received the most amount of money? 

You need to work with the `payments` and `members` tables and for now we just want the total among all years. To insert a new, blank chunk of code where you can write your beautiful code (and comments!), please use the following shortcut: `Ctrl + Alt + I` (Windows) or `cmd + option + I` (mac)

```{r}

#First I extracted the payments, and members tables.

members <- dplyr::tbl(sky_westminster, "members") %>% 
  collect()

payments <- dplyr::tbl(sky_westminster, "payments") %>% 
  collect()

#Then, I created a new table, where I joined both tables by the member id to extract total amount each member had received in donations. 

new <- members %>% 
  left_join(payments, by = c("id" ="member_id")) %>% 
  group_by(name) %>% 
  select(name, value) %>% 
  summarise(total_value = sum(value)) %>% 
  arrange(desc(total_value)) %>% 
  collect()

```


## Any `entity` that accounts for more than 5% of all donations?

Is there any `entity` whose donations account for more than 5% of the total payments given to MPs over the 2020-2022 interval? Who are they and who did they give money to?

```{r}

#First I create a table with all of the entities, add in the year as a column, filtered for the relevant years, and grouped by entity. Then I summarized the total value of the donations through the sum function. I then arranged the total value in descending order. 

entities <- payments %>% 
  mutate(year = str_sub(date, -4)) %>% 
  filter(year == c("2020", "2021", "2022")) %>% 
  group_by(entity) %>% 
  select(entity, value) %>% 
  summarise(total_value = sum(value)) %>% 
  arrange(desc(total_value))%>%
  collect()
  
#I also calculated the total value of all the donations across the three years.
total <- sum(entities$total_value)

#And then added a new column with the proportions in percentages of donations. 
prop_entities <- entities %>% 
  mutate(value_prop = (total_value/total)*100) 

#The conclusion was that Withers LLP was the only entity whose donations account for more than 5% of the total payments given to MPs over the 2020-2022 interval.

#Then I filtered to see who he donated to:

withers <- payments %>% 
  filter(entity == "Withers LLP") %>% 
  count(member_id) %>% 
  collect()

#And found that he donated to m1508, or Sir Geoffrey Cox.

```


## Do `entity` donors give to a single party or not?

- How many distinct entities who paid money to MPS are there?
- How many (as a number and %) donated to MPs belonging to a single party only?

```{r}

#To find the number of distinct entities who paid money to MPS, I used the summarize and the n_distinct function.

distinct <- payments %>% 
  summarize(unique_count = n_distinct(entity)) %>% 
  collect()

#I found that there were 2213 distinct entities.

my_parties <- members %>% 
  left_join(payments, by = c("id" ="member_id")) %>% 
  group_by(entity) %>% 
  count(party_id)

#Then to find how many donated to MPs belonging to a single party only, I counted the entities, and then filtered for those that appeared once. 

single <- my_parties %>% 
  filter(!is.na(entity)) %>% 
  count(entity) %>% 
  arrange(desc(n))

number_single <- single %>% 
  filter(n==1)

#I found that 2036 donated to MPs belonging to a single party only.

prop <- 2036/2213

#This equates to 92%.

```


## Which party has raised the greatest amount of money in each of the years 2020-2022? 

I would like you to write code that generates the following table. 

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics(here::here("images", "total_donations_table.png"), error = FALSE)

#To replicate the table, I first joined the members and payments tables, then I used the mutate function to create a new column named year, and filtered for only the relevant years. I then summed the value to derive total year donations. I also calculated the percentage the values represented within each year. 

table <- members %>% 
  left_join(payments, by = c("id" ="member_id")) %>% 
  mutate(year = str_sub(date, -4)) %>% 
  filter(year == c("2020", "2021", "2022")) %>% 
  filter(!is.na(year)) %>% 
  group_by(year, party_id) %>% 
  summarise(total_year_donations = sum(value)) %>% 
  mutate(percent = total_year_donations / sum(total_year_donations)) %>% 
  collect()

parties <- dplyr::tbl(sky_westminster, "parties") %>% 
  collect()

#I then used the left join witht the parties data to change the id for the name of each party, and arranged by year. 
           
table2 <- left_join(table, parties, by = c("party_id" = "id")) %>% 
  select(year, name, total_year_donations, percent) %>% 
  arrange(year) %>% 
  mutate(name = fct_reorder(name, -total_year_donations))

```


... and then, based on this data, plot the following graph. 

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics(here::here("images", "total_donations_graph.png"), error = FALSE)

#I then used table 2 to plot the bar graph, and followed the title and color guidelines as the picture showed. 

ggplot(table2) +
  aes(year, total_year_donations, fill = fct_reorder(name, -total_year_donations)) +
  geom_bar(stat = "identity", position = position_dodge())  +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Conservatives have captured the majority of political donations", 
       subtitle = "Donations to political parties, 2020-2022",
            x = "", 
            y = "", 
            fill = "Party")

```

This uses the default ggplot colour pallete, as I dont want you to worry about using the [official colours for each party](https://en.wikipedia.org/wiki/Wikipedia:Index_of_United_Kingdom_political_parties_meta_attributes). However, I would like you to ensure the parties are sorted according to total donations and not alphabetically. You may even want to remove some of the smaller parties that hardly register on the graph. Would facetting help you?  

```{r}

#I then created a new table to filter for when total donations are smaller than 400000, to show as other, to try a build a simpler visualisation. 

table3 <- table2 %>% 
  mutate(name = case_when(
    total_year_donations > 400000 ~ as.character(name),
    TRUE ~ "Other"
  ))  

#I then used table3 to plot the bar graph.

ggplot(table3) +
  aes(year, total_year_donations, fill = fct_reorder(name, -total_year_donations)) +
  geom_bar(stat = "identity", position = position_dodge())  +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Conservatives have captured the majority of political donations", 
       subtitle = "Donations to political parties, 2020-2022",
            x = NULL, 
            y = NULL, 
            fill = "Party")

#I also applied facet wrap to table 2 to see how it would impact the visual clarity of the data. 

ggplot(table2) +
  aes(year, total_year_donations, fill = fct_reorder(name, -total_year_donations)) +
  facet_wrap( ~name, scales="free") +
  geom_bar(stat = "identity", position = position_dodge())  +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Conservatives have captured the majority of political donations", 
       subtitle = "Donations to political parties, 2020-2022",
            x = NULL, 
            y = NULL, 
            fill = "Party")+
  theme(legend.position =  "none")

#And made another attempt with facet wrap to table 3. 

ggplot(table3) +
  aes(year, total_year_donations, fill = fct_reorder(name, -total_year_donations)) +
  facet_wrap( ~name) +
  geom_bar(stat = "identity", position = position_dodge())  +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Conservatives have captured the majority of political donations", 
       subtitle = "Donations to political parties, 2020-2022",
            x = "", 
            y = "", 
            fill = "Party")



```


Finally, when you are done working with the databse, make sure you close the connection, or disconnect from the database.

```{r}
dbDisconnect(sky_westminster)
```


# Anonymised Covid patient data from the CDC

We will be using a dataset with [anonymous Covid-19 patient data that the CDC publishes every month](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4). The file we will use was released on April 11, 2023, and has data on 98 million of patients, with 19 features. This file cannot be loaded in memory, but luckily we have the data in `parquet` format and we will use the `{arrow}` package.

## Obtain the data

The dataset `cdc-covid-geography` in in `parquet` format that {arrow}can handle. It is > 600Mb and too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false


#tic() # start timer
#cdc_data <- open_dataset(here::here("data", "cdc-covid-geography"))
#toc() # stop timer


#glimpse(cdc_data)

#This data completely broke my R studio, I tried updating the program, my software, re-downloading all the packages, and yet still nothing seems to work after having experienced what R keeps calling a fatal error. 

```
Can you query the database and replicate the following plot?

```{r echo=FALSE, out.width="100%"}
#knitr::include_graphics(here::here("images", "covid-CFR-ICU.png"), error = FALSE)
```

The previous plot is an aggregate plot for all three years of data. What if we wanted to plot Case Fatality Ratio (CFR) over time? Write code that collects the relevant data from the database and plots the following


```{r echo=FALSE, out.width="100%"}
#knitr::include_graphics(here::here("images", "cfr-icu-overtime.png"), error = FALSE)
```


For each patient, the dataframe also lists the patient's states and county [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code). The CDC also has information on the [NCHS Urban-Rural classification scheme for counties](https://www.cdc.gov/nchs/data_access/urban_rural.htm)
```{r}
#urban_rural <- read_xlsx(here::here("data", "NCHSURCodes2013.xlsx")) %>% 
  #janitor::clean_names() 
```


Each county belongs in seix diffent categoreis, with categories 1-4 being urban areas and categories 5-6 being rural, according to the following criteria captured in `x2013_code`

Category name

1. Large central metro - 1 million or more population and contains the entire population of the largest principal city
2. large fringe metro - 1 million or more poulation, but does not qualify as 1
3. Medium metro - 250K - 1 million population
4. Small metropolitan population < 250K
5. Micropolitan 
6. Noncore

Can you query the database, extract the relevant information, and reproduce the following two graphs that look at the Case Fatality ratio (CFR) in different counties, according to their population?


```{r echo=FALSE, out.width="100%"}
#knitr::include_graphics(here::here("images", "cfr-county-population.png"), error = FALSE)
```



```{r echo=FALSE, out.width="100%"}
#knitr::include_graphics(here::here("images", "cfr-rural-urban.png"), error = FALSE)
```


# Money in US politics

In the United States, [*"only American citizens (and immigrants with green cards) can contribute to federal politics, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees."*](https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs)

We will scrape and work with data foreign connected PACs that donate to US political campaigns. The data for foreign connected PAC contributions in the 2022 election cycle can be found at https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022. Then, we will use a similar approach to get data such contributions from previous years so that we can examine trends over time.

All data come from [OpenSecrets.org](https://www.opensecrets.org), a *"website tracking the influence of money on U.S. politics, and how that money affects policy and citizens' lives"*.

```{r}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

library(robotstxt)
paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

#Similar to what we did on slide 40 in the UK Cost of living exercise, I applied the read html, html nodes, and table code to get the tables that exist on our base url. 

contributions_tables <- base_url %>%
  read_html() %>% 
  html_nodes(css="table") %>% 
  html_table() 

```

- First, make sure you can scrape the data for 2022. Use janitor::clean_names() to rename variables scraped using `snake_case` naming. 

```{r}

library(janitor)

#Then to get the first table we use [[]], and then use the clean names  function to rename variables scraped using `snake_case` naming.

contributions <- contributions_tables[[1]] %>% 
  janitor::clean_names()

```


- Clean the data: 

    -   Write a function that converts contribution amounts in `total`, `dems`, and `repubs` from character strings to numeric values.
    -   Separate the `country_of_origin_parent_company` into two such that country and parent company appear in different columns for country-level analysis.

```{r}

# Created a function to parse_currency that removed dollar signs, all occurrences of commas, and converted the data type to numeric.

parse_currency <- function(x){
  x %>%
    str_remove("\\$") %>%
    str_remove_all(",") %>%
    as.numeric()
}

#Then cleaned country/parent co and contributions.

contributions <- contributions %>%
  separate(country_of_origin_parent_company, 
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  )

```

-   Write a function called `scrape_pac()` that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year. This function should

    -   have one input: the URL of the webpage and should return a data frame.
    -   add a new column to the data frame for `year`. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesn't take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the `str_sub()` function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify "last 4 characters".
    
```{r}

#Created a function to get tables that exist on url, then isolated all tables on page, parsed an html table into a dataframe, and used `tables[[1]]` to get first table, finally adding a new column with the year.



scrape_pac <- function(year) {

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/"
url <- str_c(base_url, year)
  
  contributions <- url %>% 
    read_html() %>% 
    html_element("table") %>% 
    html_table() %>% 
    janitor::clean_names()
  
  
contributions <- contributions %>%
  separate(country_of_origin_parent_company, 
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  ) %>% 
  mutate(year = year)
  
  
 return(contributions)
  
}



```


-   Construct a vector called `urls` that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year.
-   Map the `scrape_pac()` function over `urls` in a way that will result in a data frame called `contributions_all`.

```{r}
#Prepared base url and year variables
years <- seq(2000, 2022, by = 2)


#Created urls table, and applied the scrape pac function.
contributions_all <- map_df(years, scrape_pac)


```


-   Write the data frame to a csv file called `contributions-all.csv` in the `data` folder.

```{r}

#Wrote the data frame to a CSV file called `contributions-all.csv`.
write.csv(contributions_all, "contributions-all.csv")

```


# Scraping consulting jobs

The website [https://www.consultancy.uk/jobs/](https://www.consultancy.uk/jobs) lists job openings for consulting jobs.

```{r}
#| label: consulting_jobs_url
#| eval: false

library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

url_con <- "https://www.consultancy.uk/jobs/page/1"

listings_html <- url_con %>%
  read_html()

```

Identify the CSS selectors in order to extract the relevant information from this page, namely

1. job 
1. firm
1. functional area
1. type

Can you get all pages of ads, and not just the first one, `https://www.consultancy.uk/jobs/page/1` into a dataframe?

-   Write a function called `scrape_jobs()` that scrapes information from the webpage for consulting positions. This function should

    -   have one input: the URL of the webpage and should return a data frame with four columns (variables): job, firm, functional area, and type

    -   Test your function works with other pages too, e.g., https://www.consultancy.uk/jobs/page/2. Does the function seem to do what you expected it to do?

    -   Given that you have to scrape `...jobs/page/1`, `...jobs/page/2`, etc., define your URL so you can join multiple stings into one string, using `str_c()`. For instnace, if `page` is 5, what do you expect the following code to produce?
    

```{r}

#Set up my base url, pages, and the function for constructing each page's url. 

base_con <- "https://www.consultancy.uk/jobs/page/"
pages <- 1:8
url_con <- str_c(base_con, pages)

#Then I created the scrapeing function for each page.  

scrape_jobs <- function(page){
  
  base_con <- "https://www.consultancy.uk/jobs/page/"
url_con <- str_c(base_con, page)
  
  page_jobs <- url_con %>%
    read_html() %>%
    html_element("#dataTable") %>% 
    html_table()
  
  return(page_jobs)
  
}

#Tested it on page 1. 

page1 <- "https://www.consultancy.uk/jobs/page/1"
jobs_dataframe <- scrape_jobs(1)

```


-   Construct a vector called `pages` that contains the numbers for each page available

```{r}

#Constructed a vector called pages, for each page of the website, and the url_con like before. 

pages <- 1:8
all_jobs <- map_df(pages, scrape_jobs)



```


-   Map the `scrape_jobs()` function over `pages` in a way that will result in a data frame called `all_consulting_jobs`.


-   Write the data frame to a csv file called `all_consulting_jobs.csv` in the `data` folder.

```{r}

#Wrote the data frame to a CSV file called `contributions-all.csv`.
write_csv(all_jobs, "all_consulting_jobs.csv")

```


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: This week I asked everyone I could for help, especially when my R studio was crashing and burning, namely Patrick, Maxim, Abdulaziz, and João.
-   Approximately how much time did you spend on this problem set: More than I would like to admit, multiple days, most of the time I was working on making R studio work
-   What, if anything, gave you the most trouble: R studio showing fatal errors and shutting down, after I tried running the data from problem 2

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
