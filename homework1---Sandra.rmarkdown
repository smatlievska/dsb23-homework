---
title: "Homework 1"
author: "Sandra Matlievska"
date: 2023-05-09
format: 
  docx: default
  html:
    toc: true
    toc_float: true
    code-fold: true
editor: visual
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(nycflights13)
library(skimr)

```


# Data Manipulation

## Problem 1: Use logical operators to find flights that:

```         
-   Had an arrival delay of two or more hours (\> 120 minutes)
-   Flew to Houston (IAH or HOU)
-   Were operated by United (`UA`), American (`AA`), or Delta (`DL`)
-   Departed in summer (July, August, and September)
-   Arrived more than two hours late, but didn't leave late
-   Were delayed by at least an hour, but made up over 30 minutes in flight
```


```{r}
#| label: problem-1

# Had an arrival delay of two or more hours (> 120 minutes)
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(arr_delay > 120)

# Flew to Houston (IAH or HOU)
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(dest %in% c("IAH" , "HOU"))

# Were operated by United (`UA`), American (`AA`), or Delta (`DL`)
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(carrier %in% c("UA" , "AA" , "DL"))

# Departed in summer (July, August, and September)
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(month %in% c("6" , "7" , "8"))
  
# Arrived more than two hours late, but didn't leave late
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(dep_delay <= 0) %>% 
  filter(arr_delay > 120)

# Were delayed by at least an hour, but made up over 30 minutes in flight
flights %>% 
  filter(!is.na(dep_time)) %>% 
  filter(dep_delay >= 60) %>% 
  filter(arr_delay < dep_delay - 30)

```


## Problem 2: What months had the highest and lowest proportion of cancelled flights? Interpret any seasonal patterns. To determine if a flight was cancelled use the following code

<!-- -->

```         
flights %>% 
  filter(is.na(dep_time)) 
```


```{r}
#| label: problem-2

# What months had the highest and lowest % of cancelled flights?

# Aggregated the number of cancelled flights in each month.
cancelled <- flights %>% 
  filter(is.na(dep_time)) %>% 
  group_by(month) %>% 
  summarize(count = n())

# Aggregated the number of all flights in each month.
all_flights <- flights %>% 
  group_by(month) %>% 
  summarize(count = n())

# Canculated the percentage of cancelled flights from all flights.
prop <- cancelled["count"] / all_flights["count"] * 100 

# We can see that the percentage of cancelled flights is highest in February and lowest in October. Months where people tend to travel more, such as June, July, and December also display higher percentages of cancelled flights. This could indicate seasonal trends.  
```


## Problem 3: What plane (specified by the `tailnum` variable) traveled the most times from New York City airports in 2013? Please `left_join()` the resulting table with the table `planes` (also included in the `nycflights13` package).

For the plane with the greatest number of flights and that had more than 50 seats, please create a table where it flew to during 2013.


```{r}

#First, I began by filtering for the flights that originate from any of the New York City airports. Then I filter out all of the NAs in the tailnum variable. Then I use group_by, summarize, and count to derive an aggregate number of the flights from NYC for each tailnum.

times_traveled <- flights %>% 
  filter(origin %in% c("JFK" , "LGA" , "EWR")) %>% 
  filter(!is.na(tailnum)) %>% 
  group_by(tailnum) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count))

#The result shows that tailnum N725MQ has flown the most amount of times from NYC.

#Then, in order to see which tailnum has flown the most amount of times from NYC, and has over 50 seats, I first join the planes table with my times_traveled table, through the tailnum variable. Then I filter by which planes have over 50 seats, and arrange the result in descending order. Finally for visual simplicity I select to view only the tailnum, seats, and count variables.

joined <- left_join(times_traveled, planes, "tailnum") %>% 
  filter(seats > 50) %>% 
  arrange(desc(count)) %>% 
  select(tailnum, seats, count)

#From the resulting table, I can deduce that tailnum N328AA has flown from NYC the most, from all of the planes that have over 50 seats. 

#Then for this plane I created a table where we have an overview of everywhere it flew in 2013, by filtering for the plan via tail number, and for visual simplicity only selecting tailnum and dest as variables for the table. 

n328aa <- flights %>% 
  filter(tailnum == "N328AA") %>% 
  select(tailnum, dest)

```


## Problem 4: The `nycflights13` package includes a table (`weather`) that describes the weather during 2013. Use that table to answer the following questions:

```         
-   What is the distribution of temperature (`temp`) in July 2013? Identify any important outliers in terms of the `wind_speed` variable.
-   What is the relationship between `dewp` and `humid`?
-   What is the relationship between `precip` and `visib`?
```


```{r}
#Firstly, as we need to analyze the data in July 2013, I will created a new table filtering for the data for the month July.

july_weath <- weather %>% 
  filter(month == 7) 

#Then to answer Q1, and visualise the distribution of temperature in July, I created a scatterplot with geom_point categorized by the days in July. 

ggplot(july_weath, aes(x = day, y = temp)) +
  geom_point() +
  labs(title = "Temperature Distribution in July by Day", x="Day", y="Temperature")

#Alternatively, I also plotted a histogram of the temperatures of the temperatures in July. 

ggplot(july_weath)+
  aes(x=temp)+
  geom_histogram()+
  labs(title = "Temperature Distribution in July", x="Temperature", y= NULL)

#Then, I checked for outliers of the wind speed in July, by first visualising the data via  a histogram of the wind speends in July. 
ggplot(july_weath)+
  aes(x=wind_speed)+
  geom_histogram()+
  labs(title = "Wind Speed Outliers", x= "Wind Speed", y= NULL)

#From the hisogram and viewing the data, we can see that 25.31716 and 24.16638 are outliers in the data set.

#Then for question 2, to visualise the relationship between dewp and humid, I plotted a scatterplot with both variables. 

ggplot(july_weath)+
  aes(x=dewp, y =humid)+
  geom_point()+
  labs(title = "Dewp vs. Humid", x= "Dewp", y= "Humid")
  
#Although, the scatterplot did not indicate a significant correlation, I also tested the correlation via ggpairs, and found that the two variables have a correlation of 0.535.

july_weath %>% 
  select(dewp,humid) %>% 
  GGally::ggpairs()

#Then for question 3, I created another scatterplot to visualise the relationship between precip and visib.

ggplot(july_weath)+
  aes(x=visib, y=precip)+
  geom_point()+
  labs(title = "Visib vs. Precip", x= "Visib", y= "Precip")

#Additionally, I also tested via ggpairs for the correlation between precip and visib, which shows a correlation of -0.241.

july_weath %>% 
  select(visib,precip) %>% 
  GGally::ggpairs()

#Lastly, for an overview of how all the variables mentioned interacted  with one another in July, I used  ggpairs for all the variables.

july_weath %>% 
  select(dewp,humid,precip,visib,temp) %>% 
  GGally::ggpairs()

```


## Problem 5: Use the `flights` and `planes` tables to answer the following questions:

```         
-   How many planes have a missing date of manufacture?
-   What are the five most common manufacturers?
-   Has the distribution of manufacturer changed over time as reflected by the airplanes flying from NYC in 2013? (Hint: you may need to use case_when() to recode the manufacturer name and collapse rare vendors into a category called Other.)
```


```{r}

#In order to answer the first question, I first filtered for all of the planes that had a missing value (NA) for the year of manufacturing. Then, I used the summarize and count functions to find out that there are 70 planes with missing values for their manufacturing date. 

planes %>% 
  filter(is.na(year)) %>% 
  summarize(count = n())

#Then, to answer the second question I created a table that counted by the manufacturer variable, and sorted them in descending order. Additionally, I also added a new column for the proportion (%) of planes each manufacturer produced, assuming companies such as AIRBUS and AIRBUS INDUSTRIE, are separate entities. 

common_manu <- planes %>% 
  count(manufacturer, sort=TRUE) %>% 
  mutate(prop = n/sum(n) * 100)

#From the table we can see the 5 most common manufacturers are: BOEING, AIRBUS INDUSTRIE, BOMBARDIER INC, AIRBUS, EMBRAER. In order to only see the first 5 manufacturers, we can also use the head function:

head(common_manu, 5)

# First I created a new iteration of my common_manu table, where I used case_when to group all the small manufacturers that represent less than 0.001% of the planes in operation in 2013, together in a group known as "Other". Alternatively, this would represent mannufacturers with 2 planes or less still in operation in 2013.  

common_manuf <- planes %>% 
  count(manufacturer, sort=TRUE) %>% 
  mutate(prop = n/sum(n) * 100)  %>% 
  mutate(manufacturer = case_when(
    prop >= 0.001 ~ as.character(manufacturer),
    TRUE ~ "Other"
  )) %>% 
  group_by(manufacturer) %>% 
  summarize(prop = sum(prop))

#Then, for the third question, I created a table, grouped by year in order to see the changes over time. I used the count function to see how many planes each manufacturer made in each year. But I also added a new column that calculates the % of planes that are still flying in 2013, that each manufacturer created within the given year. I then used case when to recode Manufacturers that produced less than my cut-off, which I set as 2 planes, to a category "Other", for simplicity when viewing the table and graphing the data. Lastly, I grouped by manufacturer and year, and summarized the %, to show me all my relevant data points. And ensured that there were no NAs in the year by filtering them out. 

common_manu_time <- planes %>%
  group_by(year) %>% 
  count(manufacturer, sort=TRUE) %>% 
  mutate(prop = n/sum(n) * 100) %>% 
  mutate(manufacturer = case_when(
    n >= 2 ~ as.character(manufacturer),
    TRUE ~ "Other"
  )) %>% 
  group_by(manufacturer, year) %>% 
  summarize(prop = sum(prop)) %>% 
  filter(!is.na(year))

#Then, I arranged the data by year.
arr_manu <- common_manu_time %>% 
    arrange(year)

#Then I used ggplot to visualise it with a line graph, where x was the year, y the percentage of manufacturing, and the lines plotted represent each manufacturer. I also made the legend text smaller for visual simplicity. 

ggplot(arr_manu, aes(x = year, y = prop, color = manufacturer)) +
  geom_line() +
  labs(title = "Distribution of Manufacturer", x = "Year", y = "% of Manufacturing") +
  scale_color_discrete(name = "Manufacturer") +
  theme(legend.text = element_text(size = 5),
        legend.title = element_text(size = 7),
        legend.key.size = unit(0.5, "cm"))

#For visual simplicity, I also created a line graph from 1978 forward, as in almost all of the years prior (other than 1963) small manufacturers, or "other", held 100% of the production. This is likely due to the fact that from those years of manufacture typically only one plane remained in function in 2013. Thus, I first applied a filter to the data set, to filter the year from 1978 onwards.

arr_manu2 <- arr_manu %>% 
  filter(year >= 1978)

#Then, I created the line graph, where x was the year, y the percentage of manufacturing, and the lines plotted represent each manufacturer. From this we can see that the distribution has changed and that small manufacturers ("other") used to represent a larger proportion of manufacturing, alongside MCDONNELL DOUGLAS, but more recently AIRBUS, AIRBUS INDUSTRIE, BOEING, and BOMBARDIER INC represent a bigger proportion. Additionally, I made the legend text smaller for visual simplicity. 
  
ggplot(arr_manu2, aes(x = year, y = prop, color = manufacturer)) +
  geom_line() +
  labs(title = "Distribution of Manufacturer from 1978", x = "Year", y = "% of Manufacturing") +
  scale_color_discrete(name = "Manufacturer") +
  theme(legend.text = element_text(size = 5),
        legend.title = element_text(size = 7),
        legend.key.size = unit(0.5, "cm"))

```


## Problem 6: Use the `flights` and `planes` tables to answer the following questions:

```         
-   What is the oldest plane (specified by the tailnum variable) that flew from New York City airports in 2013?
-   How many airplanes that flew from New York City are included in the planes table?
```


```{r}

# To find the oldest plane, I first created a new table to extract only the tailnumbers from the flights dataset, as it also has a variable "year" which could confuse my formula in the following section.

flights_minus_year <- flights %>% 
  select(tailnum)
  
# Then I used left_join to combine the planes datset with the tailnumbers from planes flying from NYC in 2013. Then selected the two variables of interest, tailnumber and year, filtered for unique tailnumber to avoid duplicating datapoints, and arranged in ascending order by year.

new_joined <- left_join(flights_minus_year, planes, "tailnum") %>% 
  arrange(desc(year)) %>% 
  select(tailnum, year) %>% 
  unique(tailnum = TRUE) %>% 
  arrange(year) 
  
# The table shows that N381AA is the oldest plane, manufactured in year 1956.

# Then to answer question 2, regarding the number of airplanes that flew from NYC and are included in the planes table I filtered out all of the year data points that were missing when I joined the two data sets, and then applied summarize and count, to find the number of planes. 

overlaps <- new_joined %>% 
  filter(!is.na(year)) %>% 
  summarize(count = n())

# From this I found that 3252 tailnumbers were in both data sets.

```


## Problem 7: Use the `nycflights13` to answer the following questions:

```         
-   What is the median arrival delay on a month-by-month basis in each airport?
-   For each airline, plot the median arrival delay for each month and origin airport.
```


```{r}

#To see the median arrival delay for each of the airports, where the flights originate from I created a table that groups by both month and airport origin. Then, I used the summary and median function to add a column of the median of arrival delay.

delay_origin <- flights %>% 
  group_by(month, origin) %>%
  summarise(median_arr_delay = median(arr_delay, na.rm = TRUE))

#To see the median arrival delay for each of the airports, where the flights arrive I created a table that groups by both month and airport destination. Then, I used the summary and median function to add a column of the median of arrival delay.

delay_dest <- flights %>% 
  group_by(month, dest) %>%
  summarise(median_arr_delay = median(arr_delay, na.rm = TRUE))

#To see for the data for a specific airport, I could also apply a filter to the code above. For example for the origin airport EWR:

ewr <- flights %>% 
  group_by(month, origin) %>%
  summarise(median_arr_delay = median(arr_delay, na.rm = TRUE)) %>% 
  filter(origin == "EWR")

#Then for each airline, to plot the median per month, I first grouped by carrier and month, and then used the summarise and median function to calculate the median arrival delay. 

carriers_month <- flights %>% 
  group_by(carrier, month) %>%
  summarise(median_arr_delay = median(arr_delay, na.rm = TRUE)) 

#Then to plot this data in a line graph, I used geom_line, x was the month, y was the median arrival delay, and each line represented the carrier. I also labeled the graph accordingly with labs. 

ggplot(carriers_month, aes(x = month, y = median_arr_delay, color = carrier)) +
  geom_line() +
  labs(Title = "Median Arrival Delay by Carrier", x = "Month", y = "Median Arrival Delay") +
  scale_color_discrete(name = "Carrier")

#Then for each airline, to plot the median per destination airport, I first grouped by carrier and dest, and then used the summarise and median function to calculate the median arrival delay. 

carriers_origin <- flights %>% 
  group_by(carrier, origin) %>%
  summarise(median_arr_delay = median(arr_delay, na.rm = TRUE))

#Then to plot this data in a scatter plot, I used geom_point, x was the origin, y was the median arrival delay, and each line represented the carrier. I also labeled the graph accordingly with labs. 

ggplot(carriers_origin, aes(x = origin, y = median_arr_delay, color = carrier)) +
  geom_point() +
  labs(Title = "Median Origin Delay by Carrier", x = "Origin", y = "Median Arrival Delay") +
  scale_color_discrete(name = "Carrier")

```


## Problem 8: Let's take a closer look at what carriers service the route to San Francisco International (SFO). Join the `flights` and `airlines` tables and count which airlines flew the most to SFO. Produce a new dataframe, `fly_into_sfo` that contains three variables: the `name` of the airline, e.g., `United Air Lines Inc.` not `UA`, the count (number) of times it flew to SFO, and the `percent` of the trips that that particular airline flew to SFO.


```{r}

fly_into_sfo <- left_join(flights, airlines, "carrier") %>% 
  filter(dest == "SFO") %>% 
  count(name, sort = TRUE) %>% 
  mutate(prop = n / sum(n) * 100) %>% 
  select(name, n, prop)
  
```


And here is some bonus ggplot code to plot your dataframe


```{r}

fly_into_sfo %>%
  mutate(name = fct_reorder(name, prop)) %>% 
  ggplot()+
  aes(x = prop, y = name) +
  geom_col() +
  geom_text(aes(label = paste0(round(prop), "%")),
            hjust = 1,
            colour = "white",
            size = 5) +
  labs(title="Which airline dominates the NYC to SFO route?", subtitle = "as % of total flights in 2013",
  x= "Number of flights",
  y= NULL) +
  theme_minimal() + theme(plot.title.position = "plot",
      axis.text = element_text(size=12),
  plot.title = element_text(size=18)) + 
  NULL
```


## Problem 9: Let's take a look at cancellations of flights to SFO. We create a new dataframe `cancellations` as follows


```{r}

#Following instructions 

cancellations <- flights %>% 
  filter(dest == 'SFO') %>% 
  filter(is.na(dep_time))

#To answer the question I would use my cancellations dataframe, and apply ggplot to graph it. I would also need to apply facet_wrap to get the each graph for EWR and JFK organized by carrier, geom_histogram to create the histograms, set x as the month, and y as the number of cancellations of flights. And I would use labs to label the graphs.

```


I want you to think how we would organise our data manipulation to create the following plot. No need to write the code, just explain in words how you would go about it.

![](images/sfo-cancellations.png)

## Problem 10: On your own -- Hollywood Age Gap

The website https://hollywoodagegap.com is a record of *THE AGE DIFFERENCE IN YEARS BETWEEN MOVIE LOVE INTERESTS*. This is an informational site showing the age gap between movie love interests and the data follows certain rules:

-   The two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)
-   The youngest of the two actors is at least 17 years old
-   No animated characters

The age gaps dataset includes "gender" columns, which always contain the values "man" or "woman". These values appear to indicate how the characters in each film identify and some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!

The following is a data dictionary of the variables used

| variable            | class     | description                                                                                             |
|:--------------|:--------------|:------------------------------------------|
| movie_name          | character | Name of the film                                                                                        |
| release_year        | integer   | Release year                                                                                            |
| director            | character | Director of the film                                                                                    |
| age_difference      | integer   | Age difference between the characters in whole years                                                    |
| couple_number       | integer   | An identifier for the couple in case multiple couples are listed for this film                          |
| actor_1\_name       | character | The name of the older actor in this couple                                                              |
| actor_2\_name       | character | The name of the younger actor in this couple                                                            |
| character_1\_gender | character | The gender of the older character, as identified by the person who submitted the data for this couple   |
| character_2\_gender | character | The gender of the younger character, as identified by the person who submitted the data for this couple |
| actor_1\_birthdate  | date      | The birthdate of the older member of the couple                                                         |
| actor_2\_birthdate  | date      | The birthdate of the younger member of the couple                                                       |
| actor_1\_age        | integer   | The age of the older actor when the film was released                                                   |
| actor_2\_age        | integer   | The age of the younger actor when the film was released                                                 |


```{r}

age_gaps <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-14/age_gaps.csv')

#How is age_difference distributed? What's the 'typical' age_difference in movies?

ggplot(age_gaps)+
  aes(x=age_difference)+
  geom_histogram()+
  labs(title = "Age Difference Distribution", x= "Age Difference", y= NULL)
  
#The half plus seven\ rule. Large age disparities in relationships carry certain stigmas. One popular rule of thumb is the half-your-age-plus-seven rule. This rule states you should never date anyone under half your age plus seven, establishing a minimum boundary on whom one can date. In order for a dating relationship to be acceptable under this rule. How frequently does this rule apply in this dataset?

plus_seven <- age_gaps %>% 
  mutate(seven_rule = (actor_1_age/2) +7) %>% 
  filter(seven_rule>actor_2_age) %>% 
  select(actor_1_age, actor_2_age, seven_rule)

# When filtering for the half plus seven rule, we derive a table with 326 rows, thus 326 movies violate this rule.

#Which movie has the greatest number of love interests?

greatest_number <- age_gaps %>% 
  count(movie_name, sort = TRUE)

#From the table, we can deduce that Love Actually has the greates number of love interests. 

#Which actors/ actresses have the greatest number of love interests in this dataset?

actorones <- age_gaps %>% 
  count(actor_1_name, sort = TRUE)

actortwos <- age_gaps %>% 
  count(actor_2_name, sort = TRUE)

#From the tables we can see that Keanu Reeves has the largest number of love interests with 24 love interests. 

#Is the mean/median age difference staying constant over the years (1935 - 2022)?

mean_median <- age_gaps %>% 
  group_by(release_year) %>% 
  summarise(mean = mean(age_difference),
            median = median(age_difference))

ggplot(data = mean_median, aes(x = release_year, y = mean)) +
  geom_line() +
  labs(Title = "Mean Age Difference Over Time", x = "Release Year", y = "Mean Age Difference")

ggplot(data = mean_median, aes(x = release_year, y = median)) +
  geom_line() +
  labs(Title = "Median Age Difference Over Time", x = "Release Year", y = "Median Age Difference")

#From the graphs we can see that the mean and median age difference changes over time. 

#How frequently does Hollywood depict same-gender love interests?

same_gender <- age_gaps %>% 
  mutate(same = character_1_gender == character_2_gender)%>%
  filter(same==TRUE) %>% 
  count(same)

#From the table we can see that only 23 movies depict same-gender love.
```


How would you explore this data set? Here are some ideas of tables/ graphs to help you with your analysis

-   How is `age_difference` distributed? What's the 'typical' `age_difference` in movies?

-   The `half plus seven\` rule. Large age disparities in relationships carry certain stigmas. One popular rule of thumb is the [half-your-age-plus-seven](https://en.wikipedia.org/wiki/Age_disparity_in_sexual_relationships#The_.22half-your-age-plus-seven.22_rule) rule. This rule states you should never date anyone under half your age plus seven, establishing a minimum boundary on whom one can date. In order for a dating relationship to be acceptable under this rule, your partner's age must be:

$$\frac{\text{Your age}}{2} + 7 < \text{Partner Age} < (\text{Your age} - 7) * 2$$ How frequently does this rule apply in this dataset?

-   Which movie has the greatest number of love interests?
-   Which actors/ actresses have the greatest number of love interests in this dataset?
-   Is the mean/median age difference staying constant over the years (1935 - 2022)?
-   How frequently does Hollywood depict same-gender love interests?

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Render the edited and completed Quarto Markdown (qmd) file as a Word document (use the "Render" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing tour changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.

